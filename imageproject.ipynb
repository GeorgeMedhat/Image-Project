{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10209725,"sourceType":"datasetVersion","datasetId":6310061}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nimport os\nfrom tensorflow.keras import layers, models, Input\n\n# Define the color map for the 8 classes\nclass_colors = {\n    0: [0, 0, 0],  # Background clutter\n    1: [128, 0, 0],  # Building\n    2: [128, 64, 128],  # Road\n    3: [0, 128, 0],  # Tree\n    4: [128, 128, 0],  # Low vegetation\n    5: [64, 0, 128],  # Moving car\n    6: [192, 0, 192],  # Static car\n    7: [64, 64, 0],  # Human\n}\n\n\n# Function to map the class index to the corresponding RGB color\nimport numpy as np\n\ndef rgb_to_class_label_np(rgb_mask, class_mapping, color_threshold=30):\n    label_mask = np.zeros((rgb_mask.shape[0], rgb_mask.shape[1]), dtype=np.uint8)\n    \n    for label, rgb in class_mapping.items():\n        # Calculate color distance\n        color_distances = np.sqrt(\n            np.sum((rgb_mask - np.array(rgb)) ** 2, axis=-1)\n        )\n        \n        # Create mask where color distance is within threshold\n        mask = color_distances < color_threshold\n        label_mask[mask] = label\n    \n    return label_mask\n\n\n# Training data\nimage_folder = \"../input/imagedataset/IPProjectDataset24/train_data/Images\"\nmask_folder = \"../input/imagedataset/IPProjectDataset24/train_data/Labels\"\n\nval_image_folder=\"../input/imagedataset/IPProjectDataset24/val_data/Images\"\nval_mask_folder = \"../input/imagedataset/IPProjectDataset24/val_data/Labels\"\n\n\n# Get sorted file paths\nimage_paths = sorted(\n    [os.path.join(image_folder, fname) for fname in os.listdir(image_folder) if fname.endswith(('.jpg', '.png'))])\nmask_paths = sorted(\n    [os.path.join(mask_folder, fname) for fname in os.listdir(mask_folder) if fname.endswith(('.jpg', '.png'))])\n\n# Load images and masks\npillow_images = [Image.open(img_path).convert('RGB') for img_path in image_paths]  # RGB for images\npillow_masks = [Image.open(mask_path).convert('RGB') for mask_path in mask_paths]  # Grayscale for masks\n\n\n# Preprocess function\ndef preprocess_image(img, mask, target_size=(256, 256)):\n    # Resize images and masks\n    img = img.resize(target_size)\n    mask = mask.resize(target_size)\n\n    # Convert to numpy arrays\n    img = np.array(img) / 255.0  # Normalize to [0, 1]\n    mask = np.array(mask)\n\n    \n    mask = rgb_to_class_label_np(mask,class_colors)\n\n    return img, mask\n\n\n# Preprocess all images and masks\nprocessed_images = [preprocess_image(img, mask)[0] for img, mask in zip(pillow_images, pillow_masks)]\nprocessed_masks = [preprocess_image(img, mask)[1] for img, mask in zip(pillow_images, pillow_masks)]\n\n\n# Convert lists to NumPy arrays\nimages = np.array(processed_images)\nmasks = np.array(processed_masks)\n\n\n# Define augmentation layer\ndef get_augmentation_layer():\n    return tf.keras.Sequential([\n        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),  # Apply flips\n        tf.keras.layers.RandomRotation(0.2),  # Apply rotations\n        tf.keras.layers.RandomZoom(0.2)  # Apply zoom\n    ])\n\n\n# Apply augmentations to both image and mask\ndef augment_image_and_mask(img, mask):\n    # Expand mask to 3D to match image format\n    mask_expanded = np.expand_dims(mask, axis=-1)\n    \n    # Create separate augmentation layers\n    image_aug_layer = tf.keras.Sequential([\n        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n        tf.keras.layers.RandomRotation(0.2),\n        tf.keras.layers.RandomZoom(0.2)\n    ])\n    \n    mask_aug_layer = tf.keras.Sequential([\n        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n        tf.keras.layers.RandomRotation(0.2),\n        tf.keras.layers.RandomZoom(0.2)\n    ])\n    \n    # Apply augmentations\n    img_aug = image_aug_layer(tf.expand_dims(img, axis=0))[0]\n    mask_aug = mask_aug_layer(tf.expand_dims(mask_expanded, axis=0))[0]\n    \n    # Remove extra dimension and convert back to original type\n    mask_aug = mask_aug[..., 0]\n    \n    return img_aug, mask_aug\n\n\n# Apply augmentation\naugmented_images, augmented_masks = [], []\nfor img, mask in zip(images, masks):\n    img_aug, mask_aug = augment_image_and_mask(img, mask)\n    augmented_images.append(img_aug)\n    augmented_masks.append(mask_aug)\n\n# Convert augmented data to NumPy arrays\naugmented_images_np = tf.stack(augmented_images).numpy()\naugmented_masks_np = tf.stack(augmented_masks).numpy()\n\n# Combine original and augmented data\nimages = np.concatenate((images, augmented_images_np), axis=0)\nmasks = np.concatenate((masks, augmented_masks_np), axis=0)\n\n# Validation data\nval_image_paths = sorted(\n    [os.path.join(val_image_folder, fname) for fname in os.listdir(val_image_folder) if fname.endswith(('.jpg', '.png'))])\nval_mask_paths = sorted(\n    [os.path.join(val_mask_folder, fname) for fname in os.listdir(val_mask_folder) if fname.endswith(('.jpg', '.png'))])\n\n# Load validation images and masks\npillow_val_images = [Image.open(img_path).convert('RGB') for img_path in val_image_paths]  # RGB for images\npillow_val_masks = [Image.open(mask_path).convert('RGB') for mask_path in val_mask_paths]  # Grayscale for masks\n\n\n# Preprocess all validation images and masks\nprocessed_val_images = [preprocess_image(img, mask)[0] for img, mask in zip(pillow_val_images, pillow_val_masks)]\nprocessed_val_masks = [preprocess_image(img, mask)[1] for img, mask in zip(pillow_val_images, pillow_val_masks)]\n\n# Convert lists to NumPy arrays for validation\nval_images = np.array(processed_val_images)\nval_masks = np.array(processed_val_masks)\n\nprint(\"Total validation images:\", len(val_images))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T00:10:05.322795Z","iopub.execute_input":"2024-12-18T00:10:05.323108Z","iopub.status.idle":"2024-12-18T00:10:41.346590Z","shell.execute_reply.started":"2024-12-18T00:10:05.323080Z","shell.execute_reply":"2024-12-18T00:10:41.345576Z"}},"outputs":[{"name":"stdout","text":"Total validation images: 70\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from tensorflow.keras.layers import (\n    Conv2D, BatchNormalization, MaxPool2D, Input, Conv2DTranspose, \n    Concatenate, Activation, Dropout, SeparableConv2D\n)\nfrom tensorflow.keras.models import Model\n\n# Define the encoder block\ndef double_conv_block(inputs, num_filters, use_separable=False, dropout_rate=0.3):\n    ConvLayer = SeparableConv2D if use_separable else Conv2D\n    x = ConvLayer(num_filters, 3, padding=\"same\")(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(dropout_rate)(x)  # Added dropout for regularization\n\n    x = ConvLayer(num_filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\n# Define the encoder block\ndef encoder_block(inputs, num_filters, use_separable=False, dropout_rate=0.3):\n    x = double_conv_block(inputs, num_filters, use_separable, dropout_rate)\n    p = MaxPool2D((2, 2))(x)\n    return x, p\n\n# Define the decoder block\ndef decoder_block(inputs, skip, num_filters, use_separable=False, dropout_rate=0.3):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n    x = Concatenate()([x, skip])\n    x = double_conv_block(x, num_filters, use_separable, dropout_rate)\n    return x\n\n# Build the modified U-Net\ndef build_light_unet(input_shape, num_classes=8, use_separable=False, dropout_rate=0.3):\n    inputs = Input(input_shape)\n\n    # Encoder\n    s1, p1 = encoder_block(inputs, 32, use_separable, dropout_rate)  # Reduced filters\n    s2, p2 = encoder_block(p1, 64, use_separable, dropout_rate)\n    s3, p3 = encoder_block(p2, 128, use_separable, dropout_rate)\n    s4, p4 = encoder_block(p3, 256, use_separable, dropout_rate)\n\n    # Bridge\n    b1 = double_conv_block(p4, 512, use_separable, dropout_rate)\n\n    # Decoder\n    d1 = decoder_block(b1, s4, 256, use_separable, dropout_rate)\n    d2 = decoder_block(d1, s3, 128, use_separable, dropout_rate)\n    d3 = decoder_block(d2, s2, 64, use_separable, dropout_rate)\n    d4 = decoder_block(d3, s1, 32, use_separable, dropout_rate)\n\n    # Output layer\n    outputs = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(d4)\n\n    model = Model(inputs, outputs, name=\"Light_UNET\")\n    return model\n\n\n\ninput_shape = (256, 256, 3)  # Replace with the shape of your input\nnum_classes = 8  # Replace with the number of classes for segmentation\n\n# Build the modified U-Net\nmodel = build_light_unet(input_shape, num_classes, use_separable=True, dropout_rate=0.4)\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_of_classes =8\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D\nfrom tensorflow.keras.layers import Input, Conv2DTranspose, Concatenate, Activation\n# Define the encoder block\ndef double_conv_block(inputs, num_filters):\n  x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n\n  x = Conv2D(num_filters, 3, padding=\"same\")(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n\n  return x\n\ndef encoder_block(inputs, num_filters):\n  x = double_conv_block(inputs, num_filters)\n  p = MaxPool2D((2,2))(x)\n\n  return x, p\n\ndef decoder_block(inputs, skip, num_filters):\n  x = Conv2DTranspose(num_filters, (2,2), strides=2, padding=\"same\")(inputs)\n  x = Concatenate()([x, skip])\n  x = double_conv_block(x, num_filters)\n\n  return x\n\n\ndef build_unet(input_shape):\n    inputs = Input(input_shape)\n\n    # Encoder\n    s1, p1 = encoder_block(inputs, 64)\n    s2, p2 = encoder_block(p1, 128)\n    s3, p3 = encoder_block(p2, 256)\n    s4, p4 = encoder_block(p3, 512)\n\n    # Bridge\n    b1 = double_conv_block(p4, 1024)\n\n    # Decoder\n    d1 = decoder_block(b1, s4, 512)\n    d2 = decoder_block(d1, s3, 256)\n    d3 = decoder_block(d2, s2, 128)\n    d4 = decoder_block(d3, s1, 64)\n\n    # Output layer for 8 classes\n    outputs = layers.Conv2D(num_of_classes, 1, padding=\"same\", activation=\"softmax\")(d4)\n    model = models.Model(inputs, outputs, name=\"UNET\")\n    return model\ninput_shape = (256, 256, 3)  # Example input shape\nmodel = build_unet(input_shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:15:34.400343Z","iopub.execute_input":"2024-12-17T21:15:34.400720Z","iopub.status.idle":"2024-12-17T21:15:34.701471Z","shell.execute_reply.started":"2024-12-17T21:15:34.400668Z","shell.execute_reply":"2024-12-17T21:15:34.700754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Conv2DTranspose, Concatenate, Activation, BatchNormalization, UpSampling2D, Multiply, Add\nfrom tensorflow.keras.models import Model\n\n# Define the Attention Gate\ndef attention_gate(skip_connection, gating_signal, num_filters):\n    \"\"\"\n    Attention Gate (AG) to focus on relevant regions in skip connection.\n    \"\"\"\n    theta_x = Conv2D(num_filters, (1, 1), strides=(1, 1), padding='same')(skip_connection)\n    theta_x = BatchNormalization()(theta_x)\n\n    phi_g = Conv2D(num_filters, (1, 1), strides=(1, 1), padding='same')(gating_signal)\n    phi_g = BatchNormalization()(phi_g)\n\n    add = Add()([theta_x, phi_g])\n    relu = Activation('relu')(add)\n\n    psi = Conv2D(1, (1, 1), strides=(1, 1), padding='same')(relu)\n    psi = BatchNormalization()(psi)\n    psi = Activation('sigmoid')(psi)\n\n    attention_output = Multiply()([skip_connection, psi])\n\n    return attention_output\n\n\ndef double_conv_block(inputs, num_filters):\n    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    return x\n\n\ndef encoder_block(inputs, num_filters):\n    x = double_conv_block(inputs, num_filters)\n    p = MaxPool2D((2, 2))(x)\n\n    return x, p\n\n\ndef decoder_block(inputs, skip, num_filters):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n    \n    # Apply attention gate to skip connection\n    skip = attention_gate(skip, x, num_filters)\n    \n    x = Concatenate()([x, skip])\n    x = double_conv_block(x, num_filters)\n\n    return x\n\n\ndef build_attention_unet(input_shape, num_classes):\n    inputs = Input(input_shape)\n\n    # Encoder\n    s1, p1 = encoder_block(inputs, 64)\n    s2, p2 = encoder_block(p1, 128)\n    s3, p3 = encoder_block(p2, 256)\n    s4, p4 = encoder_block(p3, 512)\n\n    # Bridge\n    b1 = double_conv_block(p4, 1024)\n\n    # Decoder\n    d1 = decoder_block(b1, s4, 512)\n    d2 = decoder_block(d1, s3, 256)\n    d3 = decoder_block(d2, s2, 128)\n    d4 = decoder_block(d3, s1, 64)\n\n    outputs = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(d4)\n\n    model = Model(inputs, outputs, name=\"Attention_U-Net\")\n\n    return model\n\n\ninput_shape = (256, 256, 3) \nnum_classes = 8 \nmodel = build_attention_unet(input_shape, num_classes)\n\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel.compile(optimizer='adam',\n               loss=\"sparse_categorical_crossentropy\",\n              metrics=['accuracy'])\nmodel.summary()\n\nmodel.fit(images, masks, epochs=200, validation_data=(val_images, val_masks))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:15:41.944408Z","iopub.execute_input":"2024-12-17T21:15:41.945218Z","iopub.status.idle":"2024-12-17T22:46:51.520841Z","shell.execute_reply.started":"2024-12-17T21:15:41.945181Z","shell.execute_reply":"2024-12-17T22:46:51.519998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import (\n    Conv2D, BatchNormalization, MaxPool2D, Input, Conv2DTranspose, \n    Concatenate, Activation, Dropout, SeparableConv2D\n)\nfrom tensorflow.keras.models import Model\n\ndef double_conv_block(inputs, num_filters, use_separable=False, dropout_rate=0.3):\n    ConvLayer = SeparableConv2D if use_separable else Conv2D\n    x = ConvLayer(num_filters, 3, padding=\"same\")(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(dropout_rate)(x)\n\n    x = ConvLayer(num_filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\ndef encoder_block(inputs, num_filters, use_separable=False, dropout_rate=0.3):\n    x = double_conv_block(inputs, num_filters, use_separable, dropout_rate)\n    p = MaxPool2D((2, 2))(x)\n    return x, p\n\ndef decoder_block(inputs, skip, num_filters, use_separable=False, dropout_rate=0.3):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n    x = Concatenate()([x, skip])\n    x = double_conv_block(x, num_filters, use_separable, dropout_rate)\n    return x\n\ndef build_light_unet(input_shape, num_classes=8, use_separable=False, dropout_rate=0.3):\n    inputs = Input(input_shape)\n\n    # Encoder\n    s1, p1 = encoder_block(inputs, 32, use_separable, dropout_rate) \n    s2, p2 = encoder_block(p1, 64, use_separable, dropout_rate)\n    s3, p3 = encoder_block(p2, 128, use_separable, dropout_rate)\n    s4, p4 = encoder_block(p3, 256, use_separable, dropout_rate)\n\n    # Bridge\n    b1 = double_conv_block(p4, 512, use_separable, dropout_rate)\n\n    # Decoder\n    d1 = decoder_block(b1, s4, 256, use_separable, dropout_rate)\n    d2 = decoder_block(d1, s3, 128, use_separable, dropout_rate)\n    d3 = decoder_block(d2, s2, 64, use_separable, dropout_rate)\n    d4 = decoder_block(d3, s1, 32, use_separable, dropout_rate)\n\n    outputs = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(d4)\n\n    model = Model(inputs, outputs, name=\"Light_UNET\")\n    return model\n\n\n\ninput_shape = (256, 256, 3) \nnum_classes = 8 \n\nmodel = build_light_unet(input_shape, num_classes, use_separable=True, dropout_rate=0.4)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T18:51:15.075072Z","iopub.execute_input":"2024-12-16T18:51:15.075805Z","iopub.status.idle":"2024-12-16T18:51:15.530937Z","shell.execute_reply.started":"2024-12-16T18:51:15.075770Z","shell.execute_reply":"2024-12-16T18:51:15.530098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nimport os\nfrom tensorflow.keras import layers, models, Input\n\nclass_colors = {\n    0: [0, 0, 0],  # Background clutter\n    1: [128, 0, 0],  # Building\n    2: [128, 64, 128],  # Road\n    3: [0, 128, 0],  # Tree\n    4: [128, 128, 0],  # Low vegetation\n    5: [64, 0, 128],  # Moving car\n    6: [192, 0, 192],  # Static car\n    7: [64, 64, 0],  # Human\n}\n\n\nimport numpy as np\n\ndef rgb_to_class_label_np(rgb_mask, class_mapping, color_threshold=30):\n    label_mask = np.zeros((rgb_mask.shape[0], rgb_mask.shape[1]), dtype=np.uint8)\n    \n    for label, rgb in class_mapping.items():\n        color_distances = np.sqrt(\n            np.sum((rgb_mask - np.array(rgb)) ** 2, axis=-1)\n        )\n        \n        mask = color_distances < color_threshold\n        label_mask[mask] = label\n    \n    return label_mask\n\n\nimage_folder = \"../input/imagedataset/IPProjectDataset24/train_data/Images\"\nmask_folder = \"../input/imagedataset/IPProjectDataset24/train_data/Labels\"\n\nval_image_folder=\"../input/imagedataset/IPProjectDataset24/val_data/Images\"\nval_mask_folder = \"../input/imagedataset/IPProjectDataset24/val_data/Labels\"\n\n\nimage_paths = sorted(\n    [os.path.join(image_folder, fname) for fname in os.listdir(image_folder) if fname.endswith(('.jpg', '.png'))])\nmask_paths = sorted(\n    [os.path.join(mask_folder, fname) for fname in os.listdir(mask_folder) if fname.endswith(('.jpg', '.png'))])\n\npillow_images = [Image.open(img_path).convert('RGB') for img_path in image_paths] \npillow_masks = [Image.open(mask_path).convert('RGB') for mask_path in mask_paths]\n\ndef preprocess_image(img, mask, target_size=(256, 256)):\n    img = img.resize(target_size)\n    mask = mask.resize(target_size)\n\n    img = np.array(img) / 255.0 \n    mask = np.array(mask)\n\n    \n    mask = rgb_to_class_label_np(mask,class_colors)\n\n    return img, mask\n\nprocessed_images = [preprocess_image(img, mask)[0] for img, mask in zip(pillow_images, pillow_masks)]\nprocessed_masks = [preprocess_image(img, mask)[1] for img, mask in zip(pillow_images, pillow_masks)]\n\nimages = np.array(processed_images)\nmasks = np.array(processed_masks)\n\ndef get_augmentation_layer():\n    return tf.keras.Sequential([\n        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),  \n        tf.keras.layers.RandomRotation(0.2),  \n        tf.keras.layers.RandomZoom(0.2) \n    ])\n\ndef augment_image_and_mask(img, mask):\n    mask_expanded = np.expand_dims(mask, axis=-1)\n    image_aug_layer = tf.keras.Sequential([\n        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n        tf.keras.layers.RandomRotation(0.2),\n        tf.keras.layers.RandomZoom(0.2)\n    ])\n    \n    mask_aug_layer = tf.keras.Sequential([\n        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n        tf.keras.layers.RandomRotation(0.2),\n        tf.keras.layers.RandomZoom(0.2)\n    ])\n    \n    img_aug = image_aug_layer(tf.expand_dims(img, axis=0))[0]\n    mask_aug = mask_aug_layer(tf.expand_dims(mask_expanded, axis=0))[0]\n    \n    mask_aug = mask_aug[..., 0]\n    \n    return img_aug, mask_aug\n\n\naugmented_images, augmented_masks = [], []\nfor img, mask in zip(images, masks):\n    img_aug, mask_aug = augment_image_and_mask(img, mask)\n    augmented_images.append(img_aug)\n    augmented_masks.append(mask_aug)\n\naugmented_images_np = tf.stack(augmented_images).numpy()\naugmented_masks_np = tf.stack(augmented_masks).numpy()\n\nimages = np.concatenate((images, augmented_images_np), axis=0)\nmasks = np.concatenate((masks, augmented_masks_np), axis=0)\n\nval_image_paths = sorted(\n    [os.path.join(val_image_folder, fname) for fname in os.listdir(val_image_folder) if fname.endswith(('.jpg', '.png'))])\nval_mask_paths = sorted(\n    [os.path.join(val_mask_folder, fname) for fname in os.listdir(val_mask_folder) if fname.endswith(('.jpg', '.png'))])\n\npillow_val_images = [Image.open(img_path).convert('RGB') for img_path in val_image_paths]  \npillow_val_masks = [Image.open(mask_path).convert('RGB') for mask_path in val_mask_paths]  \n\n\nprocessed_val_images = [preprocess_image(img, mask)[0] for img, mask in zip(pillow_val_images, pillow_val_masks)]\nprocessed_val_masks = [preprocess_image(img, mask)[1] for img, mask in zip(pillow_val_images, pillow_val_masks)]\n\nval_images = np.array(processed_val_images)\nval_masks = np.array(processed_val_masks)\n\nprint(\"Total validation images:\", len(val_images))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:14:04.321683Z","iopub.execute_input":"2024-12-17T21:14:04.321981Z","iopub.status.idle":"2024-12-17T21:15:03.439461Z","shell.execute_reply.started":"2024-12-17T21:14:04.321944Z","shell.execute_reply":"2024-12-17T21:15:03.438526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef compute_confusion_matrix(y_true, y_pred, num_classes):\n    matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n    for true, pred in zip(y_true.flatten(), y_pred.flatten()):\n        matrix[true, pred] += 1\n    return matrix\n\ndef evaluate_model_extended(model, val_images, val_masks, num_classes=8):\n    predictions = model.predict(val_images)\n    predictions = np.argmax(predictions, axis=-1) \n\n    y_true = val_masks.flatten()\n    y_pred = predictions.flatten()\n\n    confusion = compute_confusion_matrix(y_true, y_pred, num_classes)\n\n    accuracy = np.sum(np.diag(confusion)) / np.sum(confusion) \n\n    iou_per_class = []\n    for c in range(num_classes):\n        intersection = confusion[c, c]\n        union = (\n            np.sum(confusion[c, :]) + np.sum(confusion[:, c]) - intersection\n        )\n        iou_per_class.append(intersection / union if union != 0 else 0)\n    mean_iou = np.mean(iou_per_class)\n\n    dice_per_class = []\n    for c in range(num_classes):\n        intersection = 2 * confusion[c, c]\n        dice = intersection / (np.sum(confusion[c, :]) + np.sum(confusion[:, c]))\n        dice_per_class.append(dice if np.sum(confusion[c, :]) != 0 else 0)\n    mean_dice = np.mean(dice_per_class)\n\n    precision_per_class = []\n    recall_per_class = []\n    for c in range(num_classes):\n        tp = confusion[c, c]\n        fp = np.sum(confusion[:, c]) - tp\n        fn = np.sum(confusion[c, :]) - tp\n        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n        precision_per_class.append(precision)\n        recall_per_class.append(recall)\n\n    mean_precision = np.mean(precision_per_class)\n    mean_recall = np.mean(recall_per_class)\n\n    total_pairs = len(y_true) * (len(y_true) - 1) / 2\n    tp_fp_fn_tn = np.sum([np.sum(confusion[c, :]) * np.sum(confusion[:, c]) for c in range(num_classes)])\n    adjusted_rand_index = (\n        (total_pairs * np.sum(np.diag(confusion)) - tp_fp_fn_tn)\n        / (total_pairs - tp_fp_fn_tn)\n    )\n\n    print(\"Evaluation Results:\")\n    print(f\"Pixel Accuracy: {accuracy:.4f}\")\n    print(f\"Mean IoU (Jaccard Index): {mean_iou:.4f}\")\n    print(f\"Mean Dice Coefficient: {mean_dice:.4f}\")\n    print(f\"Mean Precision: {mean_precision:.4f}\")\n    print(f\"Mean Recall (Sensitivity): {mean_recall:.4f}\")\n    print(f\"Adjusted Rand Index: {adjusted_rand_index:.4f}\")\n    for i, (iou, dice, precision, recall) in enumerate(zip(iou_per_class, dice_per_class, precision_per_class, recall_per_class)):\n        print(f\"Class {i}: IoU: {iou:.4f}, Dice: {dice:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:08:20.537189Z","iopub.execute_input":"2024-12-17T23:08:20.538099Z","iopub.status.idle":"2024-12-17T23:08:20.549520Z","shell.execute_reply.started":"2024-12-17T23:08:20.538059Z","shell.execute_reply":"2024-12-17T23:08:20.548836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model_extended(model, val_images, val_masks, num_classes=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:08:23.283112Z","iopub.execute_input":"2024-12-17T23:08:23.283447Z","iopub.status.idle":"2024-12-17T23:08:35.129106Z","shell.execute_reply.started":"2024-12-17T23:08:23.283415Z","shell.execute_reply":"2024-12-17T23:08:35.128241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\ndef create_mask(pred_mask):\n    pred_mask = np.argmax(pred_mask, axis=-1) \n    pred_mask = np.expand_dims(pred_mask, axis=-1) \n    return pred_mask[0]\n\ndef display(display_list):\n    plt.figure(figsize=(15, 15))\n    class_colors = [\n    (0 / 255, 0 / 255, 0 / 255),         # Background clutter (Black)\n    (128 / 255, 0 / 255, 0 / 255),       # Building (Dark Red)\n    (128 / 255, 64 / 255, 128 / 255),    # Road (Purple)\n    (0 / 255, 128 / 255, 0 / 255),       # Tree (Green)\n    (128 / 255, 128 / 255, 0 / 255),     # Low vegetation (Yellow)\n    (64 / 255, 0 / 255, 128 / 255),      # Moving car (Dark Violet)\n    (192 / 255, 0 / 255, 192 / 255),     # Static car (Pinkish Violet)\n    (64 / 255, 64 / 255, 0 / 255),       # Human (Brown)\n]\n    cmap = ListedColormap(class_colors)\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        \n        item = display_list[i]\n        if i == 0:  \n            if item.max() <= 1.0:\n                item = (item * 255).astype(np.uint8)\n            else:\n                item = item.astype(np.uint8)\n        if len(item.shape) == 2:\n            item = np.expand_dims(item, axis=-1)\n\n        plt.imshow(item.astype(np.uint8),cmap=cmap) \n        plt.axis('off')\n    plt.show()\n\ndef show_predictions(images=None, masks=None, num=1, model=None):\n    \"\"\"\n    Displays the first image of each of the num batches\n    Takes arrays of images and masks instead of a dataset\n    \"\"\"\n    if images is not None and masks is not None:\n        for i in range(num):\n            image = images[i]\n            mask = masks[i]\n            image1 = np.expand_dims(image, axis=0) \n            pred_mask = model.predict(image1)\n            display([image, mask, create_mask(pred_mask)])\n\n# Example usage:\n# Assuming `images_array` and `masks_array` are NumPy arrays of shape (num_images, height, width, channels)\nshow_predictions(val_images, val_masks, num=10, model=model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T23:10:41.617321Z","iopub.execute_input":"2024-12-17T23:10:41.618241Z","iopub.status.idle":"2024-12-17T23:10:49.378216Z","shell.execute_reply.started":"2024-12-17T23:10:41.618202Z","shell.execute_reply":"2024-12-17T23:10:49.377353Z"}},"outputs":[],"execution_count":null}]}